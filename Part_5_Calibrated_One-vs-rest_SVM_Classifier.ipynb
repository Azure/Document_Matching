{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA Matching Data Science Scenario\n",
    "\n",
    "## Part 5: \n",
    "Calibrated One-vs-rest Support Vector Machine (SVM) Classifier (Unigrams, Unigrams+Bigrams)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Part 5 of the series shows the implementation of One-vs-rest SVM Classifier. The classifier has been built using the scores learned from the _Naive Bayes Classifier_ in __Part 4__ as the feature vectors. Two feature vectors sets have been used to build the SVM classifiers: the scores learned on unigrams and the concatenation of scores learned on unigrams and scores learned on bigrams.\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.externals import joblib\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read trainQ and testQ into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_url = 'https://mezsa.blob.core.windows.net/stackoverflow/trainQwithTokens.tsv'\n",
    "testQ_url = 'https://mezsa.blob.core.windows.net/stackoverflow/testQwithTokens.tsv'\n",
    "\n",
    "trainQ = pd.read_csv(trainQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_url, sep='\\t', index_col='Id', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokens to IDs Hash\n",
    "\n",
    "For each token in the entire vocabulary, we assign it an unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Token to ID mapping: {Token: tokenId}\n",
    "def tokens_to_ids(tokens, featureHash):\n",
    "    token2IdHash = {}\n",
    "    for i in range(len(tokens)):\n",
    "        tokenList = tokens.iloc[i].split(',')\n",
    "        if featureHash is None:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys():\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "        else:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys() and t in list(featureHash.keys()):\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "            \n",
    "    return token2IdHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token2IdHashInit = tokens_to_ids(trainQ['Tokens'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the TrainQ: 4977\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique tokens in the TrainQ: \" + str(len(token2IdHashInit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Count Matrix for Each Token in Each Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_matrix(frame, token2IdHash, uniqueAnswerId):\n",
    "    # create am empty matrix with the shape of:\n",
    "    # num_row = num of unique tokens\n",
    "    # num_column = num of unique answerIds (N_wA) or num of questions in testQ (tfMatrix)\n",
    "    # rowIdx = token2IdHash.values()\n",
    "    # colIdx = index of uniqueAnswerId (N_wA) or index of questions in testQ (tfMatrix)\n",
    "    num_row = len(token2IdHash)\n",
    "    if uniqueAnswerId is not None:  # get N_wA\n",
    "        num_column = len(uniqueAnswerId)\n",
    "    else:\n",
    "        num_column = len(frame)\n",
    "    countMatrix = np.empty(shape=(num_row, num_column))\n",
    "\n",
    "    # loop through each question in the frame to fill in the countMatrix with corresponding counts\n",
    "    for i in range(len(frame)):\n",
    "        tokens = frame['Tokens'].iloc[i].split(',')\n",
    "        if uniqueAnswerId is not None:   # get N_wA\n",
    "            answerId = frame['AnswerId'].iloc[i]\n",
    "            colIdx = uniqueAnswerId.index(answerId)\n",
    "        else:\n",
    "            colIdx = i\n",
    "            \n",
    "        for t in tokens:\n",
    "            if t in token2IdHash.keys():\n",
    "                rowIdx = token2IdHash[t]\n",
    "                countMatrix[rowIdx, colIdx] += 1\n",
    "\n",
    "    return countMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get unique answerId in ascending order\n",
    "uniqueAnswerId = list(np.unique(trainQ['AnswerId']))\n",
    "# calculate the count matrix of all training questions.\n",
    "N_wAInit = count_matrix(trainQ, token2IdHashInit, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Based on Posteriori Probability P(A|w) \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/feature_selection.PNG?token=APoO9ioyiKq8Sx0dxZ5onqIzyy6ywUmEks5YnH6cwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate P(A): [P_A1, P_A2, ...]\n",
    "def prior_probability_answer(answerIds, uniqueAnswerId): \n",
    "    P_A = []\n",
    "    # convert a pandas series to a list\n",
    "    answerIds = list(answerIds)\n",
    "    \n",
    "    for id in uniqueAnswerId:\n",
    "        P_A.append(answerIds.count(id)/len(answerIds))\n",
    "    return np.array(P_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_A = prior_probability_answer(trainQ['AnswerId'], uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate P(A|w)\n",
    "def posteriori_prob(N_wAInit, P_A, uniqueAnswerId):\n",
    "    # N_A is the total number of answers\n",
    "    N_A = len(uniqueAnswerId)\n",
    "    # N_w is the total number of times w appears over all documents \n",
    "    # rowSum of count matrix (N_wAInit)\n",
    "    N_wInit = np.sum(N_wAInit, axis = 1)\n",
    "    # P(A|w) = (N_w|A + N_A * P(A))/(N_w + N_A)\n",
    "    N = N_wAInit + N_A * P_A\n",
    "    D = N_wInit + N_A\n",
    "    P_Aw = np.divide(N.T, D).T    \n",
    "    \n",
    "    return P_Aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_Aw = posteriori_prob(N_wAInit, P_A, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the top N tokens w which maximize P(A|w) for each A.\n",
    "# get FeatureHash: {token: 1}\n",
    "def feature_selection(P_Aw, token2IdHashInit, topN):\n",
    "    featureHash = {}\n",
    "    # for each answer A, sort tokens w by P(A|w)\n",
    "    sortedIdxMatrix = np.argsort(P_Aw, axis=0)[::-1]\n",
    "    # select top N tokens for each answer A\n",
    "    topMatrix = sortedIdxMatrix[0:topN, :]\n",
    "    # for each token w in topMatrix, add w to FeatureHash if it has not already been included\n",
    "    topTokenIdList = np.reshape(topMatrix, topMatrix.shape[0] * topMatrix.shape[1])\n",
    "    # get ID to Token mapping: {tokenId: Token}\n",
    "    Id2TokenHashInit = {y:x for x, y in token2IdHashInit.items()}\n",
    "    \n",
    "    for tokenId in topTokenIdList:\n",
    "        token = Id2TokenHashInit[tokenId]\n",
    "        if token not in featureHash.keys():\n",
    "            featureHash[token] = 1\n",
    "    return featureHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best top N tokens to select per answer, we have experimented different _topN_ values and found that selecting top 10 unigrams yields the best results. Two plots of evaluation matrices against different number of features are provided at the end of this notebook to describe how we determine this number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topN = 10\n",
    "featureHash = feature_selection(P_Aw, token2IdHashInit, topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-assign ID to Each Selected Token and Re-calculate Count Matrix\n",
    "\n",
    "After selecting the top N tokens of each answer, we use the collection of selected tokens for training and re-assign an ID to each selected token. Based on the new assigned IDs, we re-calculate the Count Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token2IdHash = tokens_to_ids(trainQ['Tokens'], featureHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_wA = count_matrix(trainQ, token2IdHash, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate P(w) on Full Collection of Training Questions (w is selected token)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/P_w.PNG?token=APoO9vsVzM00o3cEfOPq5mUeQ_2eJK94ks5YnH6ywA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_weights(N_wA, alpha):\n",
    "    # N_w is the total number of times w appears over all documents \n",
    "    # rowSum of count matrix (N_wA)\n",
    "    N_w = np.sum(N_wA, axis = 1)\n",
    "    # N_W is the total count of all words\n",
    "    N_W = np.sum(N_wA)\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # P(w) = (N_w + 1*alpha) / (N_W +N_V*alpha)\n",
    "    N2 = N_w + 1 * alpha\n",
    "    D2 = N_W + alpha * N_V\n",
    "    P_w = N2/D2\n",
    "\n",
    "    return P_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "P_w = feature_weights(N_wA, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Probability Function P(w|A) and P(w|NotA) on Training Data\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/probability_function.PNG?token=APoO9rWEZ1g_OgvWT_pleQlhT2DEFw3tks5YnIHzwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probability_in_answer(N_wA, P_w, beta):\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # N_WA is the total count of all words in questions on answer A \n",
    "    # colSum of count matrix (N_wA)\n",
    "    N_WA = np.sum(N_wA, axis=0)\n",
    "    # P(w|A) = (N_w|A + beta N_V P(w))/(N_W|A + beta * N_V)\n",
    "    N = (N_wA.T + beta * N_V * P_w).T\n",
    "    D = N_WA + beta * N_V\n",
    "    P_wA = N / D\n",
    "    \n",
    "    return P_wA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probability_Notin_answer(N_wA, P_w, beta):\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # N_wNotA is the count of w over all documents but not on answer A\n",
    "    # N_wNotA = N_w - N_wA\n",
    "    N_w = np.sum(N_wA, axis = 1)\n",
    "    N_wNotA = (N_w - N_wA.T).T\n",
    "    # N_WNotA is the count of all words over all documents but not on answer A\n",
    "    # N_WNotA = N_W - N_WA\n",
    "    N_W = np.sum(N_wA)\n",
    "    N_WA = np.sum(N_wA, axis=0)\n",
    "    N_WNotA = N_W - N_WA\n",
    "    # P(w|NotA) = (N_w|NotA + beta * N_V * P(w))/(N_W|NotA + beta * N_V)\n",
    "    N = (N_wNotA.T + beta * N_V * P_w).T\n",
    "    D = N_WNotA + beta * N_V\n",
    "    P_wNotA = N / D\n",
    "    \n",
    "    return P_wNotA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.0001\n",
    "P_wA = word_probability_in_answer(N_wA, P_w, beta)\n",
    "P_wNotA = word_probability_Notin_answer(N_wA, P_w, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Naive Bayes Weights\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_weight.PNG?token=APoO9s-2DejCvW03RgK6zXgiXX6UT5WWks5YnjiRwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# given an answer set A, get the NB weights for each word w: {answerId: [(word_index1, weight1), (word_index2, weight2)]}\n",
    "NBWeights = np.log(P_wA / P_wNotA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Normalized TF of Each Word w in Training and Test sets\n",
    "\n",
    "Each document/question d is typically represented by a feature vector x that represents the contents of d. Because different documents can have different lengths, it can be useful to apply L1 normalmalized feature vector x. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/tf.PNG?token=APoO9tMyEVzqoUJYT9ALcdF3_BryHHEVks5YnIQywA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_tf(frame, token2IdHash):\n",
    "    N_wQ = count_matrix(frame, token2IdHash, uniqueAnswerId=None)\n",
    "    N_WQ = np.sum(N_wQ, axis=0)\n",
    "    \n",
    "    # find the index where N_WQ is zero\n",
    "    zeroIdx = np.where(N_WQ == 0)[0]\n",
    "    \n",
    "    # if N_WQ is zero, then the x_w for that particular question would be zero.\n",
    "    # for a simple calculation, we convert the N_WQ to 1 in those cases so the demoninator is not zero. \n",
    "    if len(zeroIdx) > 0:\n",
    "        N_WQ[zeroIdx] = 1\n",
    "    \n",
    "    # x_w = P_wd = count(w)/sum(count(i in V))\n",
    "    x_w = N_wQ / N_WQ\n",
    "    \n",
    "    return x_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_wTrain = normalize_tf(trainQ, token2IdHash)\n",
    "x_wTest = normalize_tf(testQ, token2IdHash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Each Question in Test Set Against a Specific Answer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_scores.PNG?token=APoO9vABVheo1aZRkUYQq41utE6VRM1Yks5YnITBwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_A = 0\n",
    "NBScoresTrain = -beta_A + np.dot(x_wTrain.T, NBWeights)\n",
    "NBScoresTest = -beta_A + np.dot(x_wTest.T, NBWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit One-vs-Rest SVM using NB Scores and Calibrate SVM Scores into Probabiilty Estimates\n",
    "\n",
    "Traditional SVM training finds a hyperplane which maximally seperates positive and negative training tokens in a vector space. In its standard form, an SVM is a two-class classifier. To create a multi-class SVM for a problem with N_A classes, a one-versus-rest SVM classifier is typically learned for each answer class a. \n",
    "\n",
    "Firstly, we fit a linear one-vs-rest Support Vector Classifier using _svm.LinearSVC()_ from an open-source Python package **Scikit Learn**. The features are used to train the Classifier are Naive Bayes scores obtained on the training set. Like most surpervised learning methods, SVM Classifier outputs scores s(x) that can be used to rank the questions in the test set from the most probable member to the least probable member of a class a. However, those SVM scores are not equivalent to probabilities, especially in a multi-class classification case. \n",
    "\n",
    "In order to map scores into probability estimates, a parametric approach proposed by John Platt for SVM scores consists in finding the parameters A and B for a sigmoid function of the form P(s) such that the negative log-likelihood of the data is minimized.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/calibration.PNG\">\n",
    "\n",
    "**Scikit Learn** has an implementation of such probability calibration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM + Unigrams\n",
    "\n",
    "In the following section, we have experimented two _one-vs-rest SVM_ classifiers. In the first classifier, we simply use the _Naive Bayes Scores_ we have learned using unigrams as the feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NB Scores as training features\n",
    "# AnswerIds as targets\n",
    "X_train = NBScoresTrain\n",
    "Y_train = np.array(trainQ['AnswerId'])\n",
    "X_test = NBScoresTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=7,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, fit a Linear SVC model \n",
    "est = svm.LinearSVC(dual=False, multi_class='ovr', max_iter=7)\n",
    "# then fit a Calibrated Classifier with 3-fold cross-validation\n",
    "clf = CalibratedClassifierCV(est, cv=3, method='sigmoid')\n",
    "%time clf.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add model persistence\n",
    "pklFile = 'C:/Users/mez/Desktop/SVM_model.pkl'\n",
    "# save the model as an external .pkl file and load the model when it is needed\n",
    "if True: \n",
    "    joblib.dump(clf, pklFile) \n",
    "if False:\n",
    "    clf = joblib.load(pklFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a deep copy of the testQ for the second classifier\n",
    "testQ_concat = copy.deepcopy(testQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict probabilities on the test set\n",
    "Y_test_pred = clf.predict_proba(X_test)\n",
    "testQ['SVMProb'] = list(Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM + concat(Unigrams, Bigrams)\n",
    "\n",
    "In the second classifier, we concatenate the _Naive Bayes Scores_ learned using unigrams and the _Naive Bayes Scores_ learned using bigrams as the feature vectors. We have experimented various _Naive Bayes Scores_ learned using bigrams and have found that concatenating *NBScoresTrain* learned above and *NBScoresTest_bigram* learned in __Part 4__ yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload the NBScoresTrain_bigram and NBScoresTest_bigram learned from Part 4 into the current notebook\n",
    "fileNameTrain_bigram = \"C:/Users/mez/Desktop/NBScoresBigramTrain.out\"\n",
    "fileNameTest_bigram = \"C:/Users/mez/Desktop/NBScoresBigramTest.out\"\n",
    "\n",
    "# reload the text file into numpy matrix:\n",
    "if True:\n",
    "    NBScoresTrain_bigram = np.loadtxt(fileNameTrain_bigram, delimiter=',')\n",
    "    NBScoresTest_bigram = np.loadtxt(fileNameTest_bigram, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NB Scores (unigrams + bigrams) as training features\n",
    "# AnswerIds as targets\n",
    "X_train = np.concatenate((NBScoresTrain, NBScoresTrain_bigram), axis=1)\n",
    "Y_train = np.array(trainQ['AnswerId'])\n",
    "X_test = np.concatenate((NBScoresTest, NBScoresTest_bigram), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=7,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, fit a Linear SVC model \n",
    "est_concat = svm.LinearSVC(dual=False, multi_class='ovr', max_iter=7)\n",
    "# then fit a Calibrated Classifier with 3-fold cross-validation\n",
    "clf_concat = CalibratedClassifierCV(est_concat, cv=3, method='sigmoid')\n",
    "%time clf_concat.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add model persistence\n",
    "pklFile = 'C:/Users/mez/Desktop/SVM_model_concat.pkl'\n",
    "# save the model as an external .pkl file and load the model when it is needed\n",
    "if True: \n",
    "    joblib.dump(clf_concat, pklFile) \n",
    "if False:\n",
    "    clf_concat = joblib.load(pklFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict probabilities on the test set\n",
    "Y_test_pred_concat = clf_concat.predict_proba(X_test)\n",
    "testQ_concat['SVMProb'] = list(Y_test_pred_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Reload Scores\n",
    "We save the probability estimations into text files, which can be retrieved in the future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileNameTest = \"C:/Users/mez/Desktop/SVMProbsTest.out\"\n",
    "fileNameTest_concat = \"C:/Users/mez/Desktop/SVMProbsTest_concat.out\"\n",
    "\n",
    "# save scores to a text file:\n",
    "if True: \n",
    "    np.savetxt(fileNameTest, Y_test_pred, delimiter=',')\n",
    "    np.savetxt(fileNameTest_concat, Y_test_pred_concat, delimiter=',')\n",
    "\n",
    "# reload the text file into numpy matrix:\n",
    "if False:\n",
    "    Y_test_pred = np.loadtxt(fileNameTest, delimiter=',')\n",
    "    Y_test_pred_concat = np.loadtxt(fileNameTest_concat, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the Predicted Probability and Calcualte Average Rank \n",
    "\n",
    "We use two evaluation matrices to test our model performance. For each question in the test set, we calculate a calibrated probability against each answer. Then we rank the answers based on their probabilities to calculate Average Rank and Top 10 Percentage in the Test set using the below formula:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/evaluation.PNG?token=APoO9hyYDFxGc9FRbmIXU3VGv0wdeCaPks5YnIVtwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the predicted probability in descending order and map them to the corresponding AnswerId in Answer set\n",
    "def rank(frame, scores, uniqueAnswerId):\n",
    "    frame['SortedAnswers'] = list(np.array(uniqueAnswerId)[np.argsort(-scores, axis=1)])\n",
    "    \n",
    "    rankList = []\n",
    "    for i in range(len(frame)):\n",
    "        rankList.append(np.where(frame['SortedAnswers'].iloc[i] == frame['AnswerId'].iloc[i])[0][0] + 1)\n",
    "    frame['Rank'] = rankList\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testQ = rank(testQ, Y_test_pred, uniqueAnswerId)\n",
    "testQ_concat = rank(testQ_concat, Y_test_pred_concat, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the model performance from _One-vs-rest SVM Classifier_ using unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions in test set: 3671\n",
      "Total number of answers: 1275\n",
      "Total number of unique features: 3184\n",
      "Average of rank: 38.0\n",
      "Percentage of questions find answers in top 10: 0.612\n"
     ]
    }
   ],
   "source": [
    "print('Total number of questions in test set: ' + str(len(testQ)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Total number of unique features: ' + str(len(featureHash)))\n",
    "print('Average of rank: ' + str(np.floor(testQ['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ.query('Rank <= 10'))/len(testQ), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the model performance from _One-vs-rest SVM Classifier_ using the concatenation of scores learned on unigrams and scores learned on bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of rank: 46.0\n",
      "Percentage of questions find answers in top 10: 0.641\n"
     ]
    }
   ],
   "source": [
    "print('Average of rank: ' + str(np.floor(testQ_concat['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ_concat.query('Rank <= 10'))/len(testQ_concat), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Results\n",
    "\n",
    "As mentioned earlier, we plot the Average Rank and Top 10 Percentage against different numbers of features we use to train the model. We also experiment with different combination of hyperparameters (alpha, beta, and beta_A) and the best performance on this test set is obtained as below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/SVM_results.PNG\">\n",
    "\n",
    "When we train the classifier only with the Naive Bayes scores learned from unigrams, we obtain the Average Rank of 38 and the Top 10 Percentage of 61.2%. \n",
    "\n",
    "However, the feature vectors learned from bigrams doesn't seem to improve the model performance. Even we can find more correct answers for the test questions in the top 10 choices, but the classifier can only recommend the correct answer at the 46th choice in average that is worse than the result from the first classifier."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
