{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA Matching Data Science Scenario\n",
    "\n",
    "## Part 6:\n",
    "Calibrated One-vs-rest Support Vector Machine (SVM) Classifier (DSSM Feature Embeddings)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Similar to the __Part 5__, __Part 6__ of the series implements an _One-vs-rest SVM Classifier_ using the feature embeddings extracted from a [_Deep Structured Semantic Model (DSSM) Transformer_](https://microsoft.sharepoint.com/teams/TLC/SitePages/Transforms/DssmTransform.aspx) in this notebook. The transform uses pre-trained DSSM models to feature text into either a semantic embedding vector, or, given two strings, output a similarity score between them.\n",
    "\n",
    "DSSM is a neural network algorithm (see the architecture as below) that produces feature embeddings for key-value string pairs. It is trained using a dataset consisting of positive key-value pairs, from which the original rows are used as correct examples, and the strings are recombined to produce adversarial, incorrect training examples. Some example of key-value pairs include search query and clicked document title text, search query and clicked ad content text, and entity-tweet string pairs.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/DSSM.PNG\">\n",
    "\n",
    "The DSSM Transform comes with two general-purpose pre-trained models that can be used out-of-the-box to produce text featurizations in a role similar to n-gram extracting transforms. When used, it can improve prediction results on certain datasets. These models were trained using large Bing datasets, the web search model was trained using click-through data on query strings and search result titles, and the ads model was trained similarly on query strings and ad pairs. The transformer generally takes input text data as _QueryColumn_ or _DocumentColumn_. In the question answering scenario, the question (as short text form) is usually considered as _QueryColumn_ and the answer (as long text form) is usually considered as _DocumentColumn_. But it's also a good experiment to swap the two columns to test the results.\n",
    "\n",
    "In addition, the input text data can be either the raw data or pre-cleaned data.\n",
    "\n",
    "### Results\n",
    "\n",
    "For the learning purpose, we have tried 16 different experiments and their results are as below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/DSSM_results.PNG\">\n",
    "\n",
    "* Matching Approach:\n",
    "    * Q to Q: match the new question to previously seen questions, which link to their correct answers.\n",
    "    * Q to Ans: match the new question directly to answers.\n",
    "* Text Column:\n",
    "    * Text with Phrases: the text data has been pre-processed and reconstructed with the semantically meaningful phrases from __Part 1__.\n",
    "    * Raw text: the raw text data without any pre-processing.\n",
    "* Feature Extraction:\n",
    "    * Bing Ads Selection DSSM - Query Column: use the pre-trained Ads Selection model to extract feature embeddings from the last layer of neural network. The text data is considered as _QueryColumn_.\n",
    "    * Bing Web Search DSSM - Query Column: use the pre-trained Web Search model to extract feature embeddings from the last layer of neural network. The text data is considered as _QueryColumn_.\n",
    "    * Bing Ads Selection DSSM - Document Column: use the pre-trained Ads Selection model to extract feature embeddings from the last layer of neural network. The text data is considered as _DocumentColumn_.\n",
    "    * Bing Web Search DSSM - Document Column: use the pre-trained Ads Selection model to extract feature embeddings from the last layer of neural network. The text data is considered as _DocumentColumn_.\n",
    "* Model:\n",
    "    * One-vs-rest SVM: fit a calibrated one-vs-rest SVM classifier using the feature embeddings (__Part 5__ has explained the details of this type of classifier).\n",
    "    * Cosine Similarity: Compare the Cosine Similarity between two feature embeddings (__Part 2__ and __Part 3__ has explained the details of Cosine Similarity). \n",
    "\n",
    "In the first half of this notebook, we will show you the process of building the model that yields the best results in the above table.\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.externals import joblib\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: SVM + DSSM Feature Embeddings\n",
    "## Read trainQ and testQ into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/trainQwithTokens.tsv'\n",
    "testQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/testQwithTokens.tsv'\n",
    "\n",
    "trainQ = pd.read_csv(trainQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_url, sep='\\t', index_col='Id', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DSSM featurized data into DataFrames\n",
    "\n",
    "We apply the DSSM transformer on the pre-processed and reconstructed text data through the TLC GUI. Each text string is transformed to a 300-dimention feature embedding.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/TLC.PNG\">\n",
    "\n",
    "Then we output the feature embeddings to .tsv files, which are loaded into this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_embeddings_url = \"https://mezsa.blob.core.windows.net/stackoverflownew/_dssm/train_phrase_Web_Q.tsv\"\n",
    "testQ_embeddings_url = \"https://mezsa.blob.core.windows.net/stackoverflownew/_dssm/test_phrase_Web_Q.tsv\"\n",
    "\n",
    "trainQ_embeddings = pd.read_csv(trainQ_embeddings_url, sep='\\t', header=None, skiprows=7, encoding='latin1')\n",
    "testQ_embeddings = pd.read_csv(testQ_embeddings_url, sep='\\t', header=None, skiprows=7, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3777</td>\n",
       "      <td>-0.052651</td>\n",
       "      <td>0.339395</td>\n",
       "      <td>-0.712155</td>\n",
       "      <td>-0.150491</td>\n",
       "      <td>0.612713</td>\n",
       "      <td>-0.807458</td>\n",
       "      <td>-0.12581</td>\n",
       "      <td>-0.481467</td>\n",
       "      <td>-0.498498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597605</td>\n",
       "      <td>-0.052400</td>\n",
       "      <td>-0.636145</td>\n",
       "      <td>-0.888976</td>\n",
       "      <td>-0.893814</td>\n",
       "      <td>0.311533</td>\n",
       "      <td>-0.647890</td>\n",
       "      <td>-0.431012</td>\n",
       "      <td>0.117726</td>\n",
       "      <td>0.570042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3777</td>\n",
       "      <td>0.194004</td>\n",
       "      <td>0.123597</td>\n",
       "      <td>-0.649545</td>\n",
       "      <td>0.469424</td>\n",
       "      <td>0.471408</td>\n",
       "      <td>-0.911375</td>\n",
       "      <td>-0.49782</td>\n",
       "      <td>0.109869</td>\n",
       "      <td>-0.715097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>0.176386</td>\n",
       "      <td>-0.853409</td>\n",
       "      <td>-0.801949</td>\n",
       "      <td>-0.691216</td>\n",
       "      <td>0.163063</td>\n",
       "      <td>-0.604138</td>\n",
       "      <td>-0.254578</td>\n",
       "      <td>-0.149677</td>\n",
       "      <td>0.109133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6        7    \\\n",
       "0  3777 -0.052651  0.339395 -0.712155 -0.150491  0.612713 -0.807458 -0.12581   \n",
       "1  3777  0.194004  0.123597 -0.649545  0.469424  0.471408 -0.911375 -0.49782   \n",
       "\n",
       "        8         9      ...          291       292       293       294  \\\n",
       "0 -0.481467 -0.498498    ...     0.597605 -0.052400 -0.636145 -0.888976   \n",
       "1  0.109869 -0.715097    ...     0.488747  0.176386 -0.853409 -0.801949   \n",
       "\n",
       "        295       296       297       298       299       300  \n",
       "0 -0.893814  0.311533 -0.647890 -0.431012  0.117726  0.570042  \n",
       "1 -0.691216  0.163063 -0.604138 -0.254578 -0.149677  0.109133  \n",
       "\n",
       "[2 rows x 301 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first column of this dataframe represents the AnswerId\n",
    "# the rest columns represent the 300-dimension feature embeddings.\n",
    "testQ_embeddings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit One-vs-Rest SVM using DSSM Feature Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DSSM feature embeddings as training features\n",
    "# AnswerIds as targets\n",
    "X_train = np.array(trainQ_embeddings.loc[:, 1:300].astype(float))\n",
    "Y_train = np.array(trainQ_embeddings.loc[:, 0].astype(int))\n",
    "X_test = np.array(testQ_embeddings.loc[:, 1:300].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=7, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, fit a Linear SVC model \n",
    "est = svm.LinearSVC(dual=True, multi_class='ovr', max_iter=7, penalty='l2', C=1.0, loss=\"hinge\")\n",
    "# then fit a Calibrated Classifier with 3-fold cross-validation\n",
    "clf = CalibratedClassifierCV(est, cv=3, method='sigmoid')\n",
    "%time clf.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add model persistence\n",
    "pklFile = os.path.join(os.getcwd(), \"SVM_DSSM.pkl\")\n",
    "# save the model as an external .pkl file and load the model when it is needed\n",
    "if True: \n",
    "    joblib.dump(clf, pklFile) \n",
    "if False:\n",
    "    clf = joblib.load(pklFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a deep copy of the testQ for the second approach\n",
    "testQ2 = copy.deepcopy(testQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict probabilities on the test set\n",
    "Y_test_pred = clf.predict_proba(X_test)\n",
    "testQ['SVMProb'] = list(Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Reload Probability Estimates\n",
    "We save the probability estimates into text files, which can be retrieved in the future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileNameTest = os.path.join(os.getcwd(), \"SVM_DSSMTest.out\")\n",
    "\n",
    "# save scores to a text file:\n",
    "if True: \n",
    "    np.savetxt(fileNameTest, Y_test_pred, delimiter=',')\n",
    "    \n",
    "# reload the text file into numpy matrix:\n",
    "if False:\n",
    "    Y_test_pred = np.loadtxt(fileNameTest, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the Predicted Probability and Calculate Average Rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the predicted probability in descending order and map them to the corresponding AnswerId in Answer set\n",
    "def rank(frame, scores, uniqueAnswerId):\n",
    "    frame['SortedAnswers'] = list(np.array(uniqueAnswerId)[np.argsort(-scores, axis=1)])\n",
    "    \n",
    "    rankList = []\n",
    "    for i in range(len(frame)):\n",
    "        rankList.append(np.where(frame['SortedAnswers'].iloc[i] == frame['AnswerId'].iloc[i])[0][0] + 1)\n",
    "    frame['Rank'] = rankList\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get unique answerId in ascending order\n",
    "uniqueAnswerId = list(np.unique(trainQ['AnswerId']))\n",
    "testQ = rank(testQ, Y_test_pred, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions in test set: 3468\n",
      "Total number of answers: 1201\n",
      "Total number of unique features: 300\n",
      "Average of rank: 78.0\n",
      "Percentage of questions find answers in top 10: 0.535\n"
     ]
    }
   ],
   "source": [
    "print('Total number of questions in test set: ' + str(len(testQ)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Total number of unique features: ' + str(X_train.shape[1]))\n",
    "print('Average of rank: ' + str(np.floor(testQ['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ.query('Rank <= 10'))/len(testQ), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Approach 2: SVM + DSSM + NBScores\n",
    "\n",
    "In the second half, we also concatenate the same set of DSSM embeddings and Naive Bayes scores learned from unigrams in __Part 4__ to train a SVM classifier. By adding a new set of features, we want to learn whether it will improve our model predictability.\n",
    "\n",
    "## Read NBScores into a Numpy Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileNameTrain = os.path.join(os.getcwd(), \"NBScoresTrain_top4.out\")\n",
    "fileNameTest = os.path.join(os.getcwd(), \"NBScoresTest_top4.out\")\n",
    "\n",
    "# reload the text file into numpy matrix:\n",
    "if True:\n",
    "    NBScoresTrain = np.loadtxt(fileNameTrain, delimiter=',')\n",
    "    NBScoresTest = np.loadtxt(fileNameTest, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate DSSM Embeddings and NB Scores as training features\n",
    "# AnswerIds as targets\n",
    "X_train2 = np.concatenate((NBScoresTrain, np.array(trainQ_embeddings.loc[:, 1:300])), axis=1)\n",
    "Y_train2 = np.array(trainQ['AnswerId'])\n",
    "X_test2 = np.concatenate((NBScoresTest, np.array(testQ_embeddings.loc[:, 1:300])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=7, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, fit a Linear SVC model \n",
    "est2 = svm.LinearSVC(dual=True, multi_class='ovr', max_iter=7, penalty='l2', C=1.0, loss=\"hinge\")\n",
    "# then fit a Calibrated Classifier with 3-fold cross-validation\n",
    "clf2 = CalibratedClassifierCV(est2, cv=3, method='sigmoid')\n",
    "%time clf2.fit(X_train2, Y_train2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add model persistence\n",
    "pklFile2 = os.path.join(os.getcwd(), \"SVM_DSSM_NBScores.pkl\")\n",
    "# save the model as an external .pkl file and load the model when it is needed\n",
    "if True: \n",
    "    joblib.dump(clf2, pklFile2) \n",
    "if False:\n",
    "    clf2 = joblib.load(pklFile2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict probabilities on the test set\n",
    "Y_test_pred2 = clf2.predict_proba(X_test2)\n",
    "testQ2['SVMProb'] = list(Y_test_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Reload Probability Estimates\n",
    "We save the probability estimates into text files, which can be retrieved in the future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileNameTest2 = os.path.join(os.getcwd(), \"SVM_DSSM_NBScoresTest.out\")\n",
    "\n",
    "# save scores to a text file:\n",
    "if True: \n",
    "    np.savetxt(fileNameTest2, Y_test_pred2, delimiter=',')\n",
    "\n",
    "# reload the text file into numpy matrix:\n",
    "if False:\n",
    "    Y_test_pred2 = np.loadtxt(fileNameTest2, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the Predicted Probability and Calculate Average RankÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get unique answerId in ascending order\n",
    "uniqueAnswerId = list(np.unique(trainQ['AnswerId']))\n",
    "testQ2 = rank(testQ2, Y_test_pred2, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions in test set: 3468\n",
      "Total number of answers: 1201\n",
      "Total number of unique features: 1501\n",
      "Average of rank: 32.0\n",
      "Percentage of questions find answers in top 10: 0.657\n"
     ]
    }
   ],
   "source": [
    "print('Total number of questions in test set: ' + str(len(testQ2)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Total number of unique features: ' + str(X_train2.shape[1]))\n",
    "print('Average of rank: ' + str(np.floor(testQ2['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ2.query('Rank <= 10'))/len(testQ2), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As we can easily observed that adding NB Scores (unigrams) to the feature space does help improve the model performance. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
