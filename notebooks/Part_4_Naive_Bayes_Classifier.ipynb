{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA Matching Data Science Scenario\n",
    "\n",
    "## Part 4: Naive Bayes Classifier (Unigrams and Bigrams)\n",
    "\n",
    "### Overview\n",
    "\n",
    "__Part 4__ of the series shows the implementation of the _Naive Bayes Classifier_ as described in the paper entitled [\"MCE Training Techniques for Topic Identification of Spoken Audio Documents\"](http://ieeexplore.ieee.org/abstract/document/5742980/).\n",
    "\n",
    "In this notebook, we have trained two classifiers. The first classifier is trained on a collection of unigrams, which could be single words or learned phrases from the __Part 1__. The second classifier is trained on a collection of bigrams, which could be the concatenation of single words or learned phrases. For example, the bag of bigrams of the sentence \"create a javascript object\" are #_create, create_a, a_javascript, javascript_object, object_# after adding the sentence boundary symbol #.\n",
    "\n",
    "At the end, we save the Naive Bayes scores obtained on both training and test datasets into text files, which can be retrieved in the future notebooks.\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from azure.storage import CloudStorageAccount\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read trainQ and testQ into Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/trainQwithTokens.tsv'\n",
    "testQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/testQwithTokens.tsv'\n",
    "\n",
    "trainQ = pd.read_csv(trainQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_url, sep='\\t', index_col='Id', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokens to IDs Hash\n",
    "\n",
    "We assign an unique ID to each token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of ngrams [ngram1, ngram2, ...]\n",
    "def create_ngram(tokens, ngram):\n",
    "    if ngram > 1:\n",
    "        # split tokens into a list\n",
    "        # add \"#\" to represent the benginning and the end of sentence\n",
    "        tokenList = [\"#\"] + tokens.split(',') + [\"#\"]\n",
    "        # create a list of ngrams\n",
    "        ngramList = [\"_\".join(tokenList[i:i+ngram]) for i in range(len(tokenList) - (ngram-1))]\n",
    "    else: \n",
    "        # If ngram = 1, then only unigram will be considered\n",
    "        ngramList = tokens.split(',')\n",
    "\n",
    "    return ngramList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Token to ID mapping: {Token: tokenId}\n",
    "def tokens_to_ids(tokens, featureHash, ngram):\n",
    "    token2IdHash = {}\n",
    "    for i in range(len(tokens)):\n",
    "        # If ngram = 1, then only unigram will be considered\n",
    "        tokenList = create_ngram(tokens.iloc[i], ngram)\n",
    "            \n",
    "        if featureHash is None:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys():\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "        else:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys() and t in list(featureHash.keys()):\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "            \n",
    "    return token2IdHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token2IdHashInit = tokens_to_ids(trainQ['Tokens'], featureHash=None, ngram=1)\n",
    "token2IdHashInit_bigram = tokens_to_ids(trainQ['Tokens'], featureHash=None, ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique unigrams in the TrainQ: 4848\n",
      "Total number of unique bigrams in the TrainQ: 184469\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique unigrams in the TrainQ: \" + str(len(token2IdHashInit)))\n",
    "print(\"Total number of unique bigrams in the TrainQ: \" + str(len(token2IdHashInit_bigram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Count Matrix for Each Token in Each Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_matrix(frame, token2IdHash, uniqueAnswerId, ngram):\n",
    "    # create am empty matrix with the shape of:\n",
    "    # num_row = num of unique tokens\n",
    "    # num_column = num of unique answerIds (N_wA) or num of questions in testQ (tfMatrix)\n",
    "    # rowIdx = token2IdHash.values()\n",
    "    # colIdx = index of uniqueAnswerId (N_wA) or index of questions in testQ (tfMatrix)\n",
    "    num_row = len(token2IdHash)\n",
    "    if uniqueAnswerId is not None:  # get N_wA\n",
    "        num_column = len(uniqueAnswerId)\n",
    "    else:\n",
    "        num_column = len(frame)\n",
    "    countMatrix = np.empty(shape=(num_row, num_column))\n",
    "\n",
    "    # loop through each question in the frame to fill in the countMatrix with corresponding counts\n",
    "    for i in range(len(frame)):\n",
    "        # If ngram = 1, then only unigram will be considered\n",
    "        tokens = create_ngram(frame['Tokens'].iloc[i], ngram)\n",
    "            \n",
    "        if uniqueAnswerId is not None:   # get N_wA\n",
    "            answerId = frame['AnswerId'].iloc[i]\n",
    "            colIdx = uniqueAnswerId.index(answerId)\n",
    "        else:\n",
    "            colIdx = i\n",
    "            \n",
    "        for t in tokens:\n",
    "            if t in token2IdHash.keys():\n",
    "                rowIdx = token2IdHash[t]\n",
    "                countMatrix[rowIdx, colIdx] += 1\n",
    "\n",
    "    return countMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get unique answerId in ascending order\n",
    "uniqueAnswerId = list(np.unique(trainQ['AnswerId']))\n",
    "# calculate the count matrix of all training questions (unigrams only).\n",
    "N_wAInit = count_matrix(trainQ, token2IdHashInit, uniqueAnswerId, ngram=1)\n",
    "# calculate the count matrix of all training questions (bigrams only).\n",
    "N_wAInit_bigram = count_matrix(trainQ, token2IdHashInit_bigram, uniqueAnswerId, ngram=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Based on Posteriori Probability P(A|w) \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/feature_selection.PNG?token=APoO9ioyiKq8Sx0dxZ5onqIzyy6ywUmEks5YnH6cwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate P(A): [P_A1, P_A2, ...]\n",
    "def prior_probability_answer(answerIds, uniqueAnswerId): \n",
    "    P_A = []\n",
    "    # convert a pandas series to a list\n",
    "    answerIds = list(answerIds)\n",
    "    \n",
    "    for id in uniqueAnswerId:\n",
    "        P_A.append(answerIds.count(id)/len(answerIds))\n",
    "    return np.array(P_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_A = prior_probability_answer(trainQ['AnswerId'], uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate P(A|w)\n",
    "def posteriori_prob(N_wAInit, P_A, uniqueAnswerId):\n",
    "    # N_A is the total number of answers\n",
    "    N_A = len(uniqueAnswerId)\n",
    "    # N_w is the total number of times w appears over all documents \n",
    "    # rowSum of count matrix (N_wAInit)\n",
    "    N_wInit = np.sum(N_wAInit, axis = 1)\n",
    "    # P(A|w) = (N_w|A + N_A * P(A))/(N_w + N_A)\n",
    "    N = N_wAInit + N_A * P_A\n",
    "    D = N_wInit + N_A\n",
    "    P_Aw = np.divide(N.T, D).T    \n",
    "    \n",
    "    return P_Aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_Aw = posteriori_prob(N_wAInit, P_A, uniqueAnswerId)\n",
    "P_Aw_bigram = posteriori_prob(N_wAInit_bigram, P_A, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the top N tokens w which maximize P(A|w) for each A.\n",
    "# get FeatureHash: {token: 1}\n",
    "def feature_selection(P_Aw, token2IdHashInit, topN):\n",
    "    featureHash = {}\n",
    "    # for each answer A, sort tokens w by P(A|w)\n",
    "    sortedIdxMatrix = np.argsort(P_Aw, axis=0)[::-1]\n",
    "    # select top N tokens for each answer A\n",
    "    topMatrix = sortedIdxMatrix[0:topN, :]\n",
    "    # for each token w in topMatrix, add w to FeatureHash if it has not already been included\n",
    "    topTokenIdList = np.reshape(topMatrix, topMatrix.shape[0] * topMatrix.shape[1])\n",
    "    # get ID to Token mapping: {tokenId: Token}\n",
    "    Id2TokenHashInit = {y:x for x, y in token2IdHashInit.items()}\n",
    "    \n",
    "    for tokenId in topTokenIdList:\n",
    "        token = Id2TokenHashInit[tokenId]\n",
    "        if token not in featureHash.keys():\n",
    "            featureHash[token] = 1\n",
    "    return featureHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best top N tokens to select per answer, we have experimented different _topN_ values and found that selecting top 3 unigrams and top 30 bigrams per answer yields the best restuls. Four plots of evaluation matrices against different number of features are provided at the end of this notebook to describe how we determine those two numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureHash = feature_selection(P_Aw, token2IdHashInit, topN=3)\n",
    "featureHash_bigram = feature_selection(P_Aw_bigram, token2IdHashInit_bigram, topN=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-assign ID to Each Selected Token and Re-calculate Count Matrix\n",
    "\n",
    "After selecting the top N tokens of each answer, we use the collection of selected tokens for training and re-assign an ID to each selected token. Based on the new assigned IDs, we re-calculate the Count Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.53 s\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%time token2IdHash = tokens_to_ids(trainQ['Tokens'], featureHash=featureHash, ngram=1)\n",
    "%time token2IdHash_bigram = tokens_to_ids(trainQ['Tokens'], featureHash=featureHash_bigram, ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_wA = count_matrix(trainQ, token2IdHash, uniqueAnswerId, ngram=1)\n",
    "N_wA_bigram = count_matrix(trainQ, token2IdHash_bigram, uniqueAnswerId, ngram=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate P(w) on Full Collection of Training Questions (w is selected token)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/P_w.PNG?token=APoO9vsVzM00o3cEfOPq5mUeQ_2eJK94ks5YnH6ywA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_weights(N_wA, alpha):\n",
    "    # N_w is the total number of times w appears over all documents \n",
    "    # rowSum of count matrix (N_wA)\n",
    "    N_w = np.sum(N_wA, axis = 1)\n",
    "    # N_W is the total count of all words\n",
    "    N_W = np.sum(N_wA)\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # P(w) = (N_w + 1*alpha) / (N_W +N_V*alpha)\n",
    "    N2 = N_w + 1 * alpha\n",
    "    D2 = N_W + alpha * N_V\n",
    "    P_w = N2/D2\n",
    "\n",
    "    return P_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "P_w = feature_weights(N_wA, alpha)\n",
    "P_w_bigram = feature_weights(N_wA_bigram, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Probability Function P(w|A) and P(w|NotA) on Training Data\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/probability_function.PNG?token=APoO9rWEZ1g_OgvWT_pleQlhT2DEFw3tks5YnIHzwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probability_in_answer(N_wA, P_w, beta):\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # N_WA is the total count of all words in questions on answer A \n",
    "    # colSum of count matrix (N_wA)\n",
    "    N_WA = np.sum(N_wA, axis=0)\n",
    "    # P(w|A) = (N_w|A + beta N_V P(w))/(N_W|A + beta * N_V)\n",
    "    N = (N_wA.T + beta * N_V * P_w).T\n",
    "    D = N_WA + beta * N_V\n",
    "    P_wA = N / D\n",
    "    \n",
    "    return P_wA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probability_Notin_answer(N_wA, P_w, beta):\n",
    "    # N_V is the count of unique words in the vocabulary\n",
    "    N_V = N_wA.shape[0]\n",
    "    # N_wNotA is the count of w over all documents but not on answer A\n",
    "    # N_wNotA = N_w - N_wA\n",
    "    N_w = np.sum(N_wA, axis = 1)\n",
    "    N_wNotA = (N_w - N_wA.T).T\n",
    "    # N_WNotA is the count of all words over all documents but not on answer A\n",
    "    # N_WNotA = N_W - N_WA\n",
    "    N_W = np.sum(N_wA)\n",
    "    N_WA = np.sum(N_wA, axis=0)\n",
    "    N_WNotA = N_W - N_WA\n",
    "    # P(w|NotA) = (N_w|NotA + beta * N_V * P(w))/(N_W|NotA + beta * N_V)\n",
    "    N = (N_wNotA.T + beta * N_V * P_w).T\n",
    "    D = N_WNotA + beta * N_V\n",
    "    P_wNotA = N / D\n",
    "    \n",
    "    return P_wNotA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.0001\n",
    "# unigrams:\n",
    "P_wA = word_probability_in_answer(N_wA, P_w, beta)\n",
    "P_wNotA = word_probability_Notin_answer(N_wA, P_w, beta)\n",
    "# bigrams:\n",
    "P_wA_bigram = word_probability_in_answer(N_wA_bigram, P_w_bigram, beta)\n",
    "P_wNotA_bigram = word_probability_Notin_answer(N_wA_bigram, P_w_bigram, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Naive Bayes Weights\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_weight.PNG?token=APoO9s-2DejCvW03RgK6zXgiXX6UT5WWks5YnjiRwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# given an answer set A, get the NB weights for each word w: {answerId: [(word_index1, weight1), (word_index2, weight2)]}\n",
    "NBWeights = np.log(P_wA / P_wNotA)\n",
    "NBWeights_bigram = np.log(P_wA_bigram / P_wNotA_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Normalized TF of Each Word w in Test Set\n",
    "\n",
    "Each document/question d is typically represented by a feature vector x that represents the contents of d. Because different documents can have different lengths, it can be useful to apply L1 normalized feature vector x. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/tf.PNG?token=APoO9tMyEVzqoUJYT9ALcdF3_BryHHEVks5YnIQywA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_tf(frame, token2IdHash, ngram):\n",
    "    N_wQ = count_matrix(frame, token2IdHash, uniqueAnswerId=None, ngram=ngram)\n",
    "    N_WQ = np.sum(N_wQ, axis=0)\n",
    "    \n",
    "    # find the index where N_WQ is zero\n",
    "    zeroIdx = np.where(N_WQ == 0)[0]\n",
    "    \n",
    "    # if N_WQ is zero, then the x_w for that particular question would be zero.\n",
    "    # for a simple calculation, we convert the N_WQ to 1 in those cases so the demoninator is not zero. \n",
    "    if len(zeroIdx) > 0:\n",
    "        N_WQ[zeroIdx] = 1\n",
    "    \n",
    "    # x_w = P_wd = count(w)/sum(count(i in V))\n",
    "    x_w = N_wQ / N_WQ\n",
    "    \n",
    "    return x_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unigrams:\n",
    "x_wTrain = normalize_tf(trainQ, token2IdHash, ngram=1)\n",
    "x_wTest = normalize_tf(testQ, token2IdHash, ngram=1)\n",
    "\n",
    "# bigrams:\n",
    "x_wTrain_bigram = normalize_tf(trainQ, token2IdHash_bigram, ngram=2)\n",
    "x_wTest_bigram = normalize_tf(testQ, token2IdHash_bigram, ngram=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Each Question in Test Set Against a Specific Answer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_scores.PNG?token=APoO9vABVheo1aZRkUYQq41utE6VRM1Yks5YnITBwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_A = 0\n",
    "# unigrams:\n",
    "NBScoresTrain = -beta_A + np.dot(x_wTrain.T, NBWeights)\n",
    "NBScoresTest = -beta_A + np.dot(x_wTest.T, NBWeights)\n",
    "\n",
    "# bigrams:\n",
    "NBScoresTrain_bigram = -beta_A + np.dot(x_wTrain_bigram.T, NBWeights_bigram)\n",
    "NBScoresTest_bigram = -beta_A + np.dot(x_wTest_bigram.T, NBWeights_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Reload Scores\n",
    "We save the Naive Bayes scores into text files, which can be retrieved in the future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileNameTrain = os.path.join(os.getcwd(), \"NBScoresTrain.out\")\n",
    "fileNameTest = os.path.join(os.getcwd(), \"NBScoresTest.out\")\n",
    "\n",
    "fileNameTrain_bigram = os.path.join(os.getcwd(), \"NBScoresBigramTrain.out\")\n",
    "fileNameTest_bigram = os.path.join(os.getcwd(), \"NBScoresBigramTest.out\")\n",
    "\n",
    "# save scores to a text file:\n",
    "if True: \n",
    "    np.savetxt(fileNameTrain, NBScoresTrain, delimiter=',')\n",
    "    np.savetxt(fileNameTest, NBScoresTest, delimiter=',')\n",
    "    np.savetxt(fileNameTrain_bigram, NBScoresTrain_bigram, delimiter=',')\n",
    "    np.savetxt(fileNameTest_bigram, NBScoresTest_bigram, delimiter=',')\n",
    "\n",
    "# reload the text file into numpy matrix:\n",
    "if False:\n",
    "    NBScoresTrain = np.loadtxt(fileNameTrain, delimiter=',')\n",
    "    NBScoresTest = np.loadtxt(fileNameTest, delimiter=',')\n",
    "    NBScoresTrain_bigram = np.loadtxt(fileNameTrain_bigram, delimiter=',')\n",
    "    NBScoresTest_bigram = np.loadtxt(fileNameTest_bigram, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the Naive Bayes Scores and Calculate Average Rank \n",
    "\n",
    "We use two evaluation matrices to test our model performance. For each question in the test set, we calculate a __Naive Bayes Score__ against each answer. Then we rank the answers based on their __Naive Bayes Scores__ to calculate __Average Rank__ and __Top 10 Percentage__ in the Test set using the below formula:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/evaluation.PNG?token=APoO9hyYDFxGc9FRbmIXU3VGv0wdeCaPks5YnIVtwA%3D%3D\">\n",
    "\n",
    "The __Average Rank__ can be interpreted as in average at which position we can find the correct answer among all available answers for a given question. \n",
    "\n",
    "The __Top 10 Percentage__ can be interpreted as how many percentage of the new questions that we can find their correct answers in the first 10 choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the similarity scores in descending order and map them to the corresponding AnswerId in Answer set\n",
    "def rank(frame, scores, uniqueAnswerId):\n",
    "    frame['SortedAnswers'] = list(np.array(uniqueAnswerId)[np.argsort(-scores, axis=1)])\n",
    "    \n",
    "    rankList = []\n",
    "    for i in range(len(frame)):\n",
    "        rankList.append(np.where(frame['SortedAnswers'].iloc[i] == frame['AnswerId'].iloc[i])[0][0] + 1)\n",
    "    frame['Rank'] = rankList\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testQ_bigram = copy.deepcopy(testQ)\n",
    "testQ = rank(testQ, NBScoresTest, uniqueAnswerId)\n",
    "testQ_bigram = rank(testQ_bigram, NBScoresTest_bigram, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the model performance from _Naive Bayes Classifier_ using unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions in test set: 3468\n",
      "Total number of answers: 1201\n",
      "Total number of unique features: 1396\n",
      "Average of rank: 41.0\n",
      "Percentage of questions find answers in top 10: 0.631\n"
     ]
    }
   ],
   "source": [
    "print('Total number of questions in test set: ' + str(len(testQ)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Total number of unique features: ' + str(len(featureHash)))\n",
    "print('Average of rank: ' + str(np.floor(testQ['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ.query('Rank <= 10'))/len(testQ), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the model performance from _Naive Bayes Classifier_ using bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of rank: 335.0\n",
      "Percentage of questions find answers in top 10: 0.42\n"
     ]
    }
   ],
   "source": [
    "print('Average of rank: ' + str(np.floor(testQ_bigram['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ_bigram.query('Rank <= 10'))/len(testQ_bigram), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Results\n",
    "\n",
    "As mentioned earlier, we plot the __Average Rank__ and __Top 10 Percentage__ against different numbers of features we use to train the models. We also experiment with different combinations of hyperparameters (alpha, beta, and beta_A) and the best performance on this test set is obtained as below.\n",
    "\n",
    "By implementing the Naive Bayes Classifier using a collection of unigrams, we have improved the __Average Rank__ from 65 (Part 3) to 41 and __Top 10 Percentage__ from 39.9% (Part 3) to 63.1%. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_results.PNG\">\n",
    "\n",
    "The Naive Bayes Classifier built using a collection of bigrams seem to perform poorly. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/NB_bigram_results.PNG\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
