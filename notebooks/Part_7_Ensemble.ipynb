{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA Matching Data Science Scenario\n",
    "\n",
    "## Part 7: \n",
    "Ensemble Model\n",
    "\n",
    "### Overview\n",
    "\n",
    "__Part 7__ of the series shows a ensemble model that combine the predicted probabilities or scores from 5 base classifiers. We combine the base classifiers based on a weighted average of the base classifiers learned in previous Parts.\n",
    "* Calibrated Na√Øve Bayes Scores (train a Logistic Regression model on the Naive Bayes Scores to obtain probabilities)\n",
    "* Calibrated linear SVM learned with NB Scores (Unigrams)\n",
    "* Calibrated linear SVM learned with NB Scores (Unigrams + Bigrams)\n",
    "* Calibrated linear SVM learned with DSSM Embeddings\n",
    "* Calibrated linear SVM learned with DSSM Embeddings and NB Scores (Unigrams)\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.externals import joblib  \n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read trainQ and testQ into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_url = 'https://mezsa.blob.core.windows.net/stackoverflow/trainQwithTokens.tsv'\n",
    "testQ_url = 'https://mezsa.blob.core.windows.net/stackoverflow/testQwithTokens.tsv'\n",
    "\n",
    "trainQ = pd.read_csv(trainQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_url, sep='\\t', index_col='Id', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Probabilities or Scores from 5 Base Classifier into Numpy Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NBScores (unigrams):\n",
    "NBScoresTrain = np.loadtxt(os.path.join(os.getcwd(), \"NBScoresTrain_top10.out\"), delimiter=',')\n",
    "NBScoresTest = np.loadtxt(os.path.join(os.getcwd(), \"NBScoresTest_top10.out\"), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Probabilities learned with NB Scores (Unigrams):\n",
    "SVMProbsTest = np.loadtxt(os.path.join(os.getcwd(), \"SVMProbsTest.out\"), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM Probabilities learned with NB Scores (Unigrams + Bigrams):\n",
    "SVMProbsTest_concat = np.loadtxt(os.path.join(os.getcwd(), \"SVMProbsTest_concat.out\"), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Probabilities learned with DSSM Embeddings:\n",
    "SVM_DSSMTest = np.loadtxt(os.path.join(os.getcwd(), \"SVM_DSSMTest.out\"), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Probabilities learned with DSSM Embeddings and NB Scores (Unigrams):\n",
    "SVM_DSSM_NBScoresTest = np.loadtxt(os.path.join(os.getcwd(), \"SVM_DSSM_NBScoresTest.out\"), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate NB Scores into Probabilities\n",
    "\n",
    "As the calibrated SVM classifiers provide probabilities, we want convert the NB scores into probabilities to keep the inputs of the ensemble model at the same scale. For this conversion, we fit a _logistic Regression_ model on the observed NB Scores to predict the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NB Scores as training features\n",
    "# AnswerIds as targets\n",
    "X_train = NBScoresTrain\n",
    "Y_train = np.array(trainQ['AnswerId'])\n",
    "X_test = NBScoresTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=20, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression model on the classifier outputs.\n",
    "# Note: this model may take a while to complete.\n",
    "lr = LR(multi_class='ovr', solver='sag', max_iter=20)                                                       \n",
    "%time lr.fit(X_train, Y_train)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add model persistence\n",
    "pklFile = os.path.join(os.getcwd(), \"LR_NBScores.pkl\")\n",
    "# save the model as an external .pkl file and load the model when it is needed\n",
    "if True: \n",
    "    joblib.dump(lr, pklFile) \n",
    "if False:\n",
    "    lr = joblib.load(pklFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the probability estimates\n",
    "NBScoresTest_calibrated = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Weights to Each Model\n",
    "\n",
    "We have performed a Grid Search by assigning each base classifier a weight that follows uniform distribution and we have found that the best results are observed when the 5 classifiers are equally weighted. Therefore, we will give a 0.2 as the weight to each classifier below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_pred = 0.2*NBScoresTest_calibrated + 0.2*SVMProbsTest + 0.2*SVMProbsTest_concat + 0.2*SVM_DSSMTest + 0.2*SVM_DSSM_NBScoresTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the Weighted Average of Probability and Calculate Average Rank \n",
    "\n",
    "We use two evaluation matrices to test our model performance. For each question in the test set, we calculate a weighted average of the probabilities obtained from the base classifiers against each answer. Then we rank the answers based on their weighted average to calculate __Average Rank__ and __Top 10 Percentage__ in the Test set using the below formula:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/evaluation.PNG?token=APoO9hyYDFxGc9FRbmIXU3VGv0wdeCaPks5YnIVtwA%3D%3D\">\n",
    "\n",
    "The __Average Rank__ can be interpreted as in average at which position we can find the correct answer among all available answers for a given question. \n",
    "\n",
    "The __Top 10 Percentage__ can be interpreted as how many percentage of the new questions that we can find their correct answers in the first 10 choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the predicted probability in descending order and map them to the corresponding AnswerId in Answer set\n",
    "def rank(frame, scores, uniqueAnswerId):\n",
    "    frame['SortedAnswers'] = list(np.array(uniqueAnswerId)[np.argsort(-scores, axis=1)])\n",
    "    \n",
    "    rankList = []\n",
    "    for i in range(len(frame)):\n",
    "        rankList.append(np.where(frame['SortedAnswers'].iloc[i] == frame['AnswerId'].iloc[i])[0][0] + 1)\n",
    "    frame['Rank'] = rankList\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get unique answerId in ascending order\n",
    "uniqueAnswerId = list(np.unique(trainQ['AnswerId']))\n",
    "testQ = rank(testQ, Y_test_pred, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions in test set: 3671\n",
      "Total number of answers: 1275\n",
      "Average of rank: 31.0\n",
      "Percentage of questions find answers in top 10: 0.642\n"
     ]
    }
   ],
   "source": [
    "print('Total number of questions in test set: ' + str(len(testQ)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Average of rank: ' + str(np.floor(testQ['Rank'].mean())))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ.query('Rank <= 10'))/len(testQ), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Analysis of the Training Example Size\n",
    "\n",
    "In our experiment, we have noticed that some Answer class only contains a very few number of training example. As we have built One-vs-rest Support Vector Machine classifiers in the most of our experiments, those classes with very few training examples convey imbalanced datasets. Training a classifier on a small amount of examples is not sufficient. Therefore, we have performed an analysis to study how the size of training example per class actually impact on the model performance.\n",
    "\n",
    "In this study, we test the __Average Rank__ and __Top 10 Percentage__ distribution with different numbers of training examples per class. As we can see from the distribution below, our ensemble model can secure an __Average Rank__ less than 20 (out of 1275 different answer classes) and a __Top 10 Percentage__ over 60% when we have more than 15 training examples per class.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/training_size.PNG\">\n",
    "\n",
    "Even the number of classes that have more than 15 training examples are very limited in our particular example. But this study is very meaningful for future works as we have learned the training example size is very critical and having a decent number of training examples per class is a must have.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/training_size_details.PNG\">\n",
    "\n",
    "With the above study, we have decided to only consider the answer classes that have more than 13 training examples that reduces the entire dataset to 5423 training examples, 1832 test examples, and 109 unique answer classes. By using this subset of training and test datasets, we have replicated the same ensemble modeling process described in this notebook and obtained an __Average Rank__ of 4 and __Top 10 Percentage__ of 91%.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/subset_results.PNG\">\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
