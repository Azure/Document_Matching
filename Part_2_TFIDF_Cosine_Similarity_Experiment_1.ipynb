{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA Matching Data Science Scenario\n",
    "\n",
    "## Part 2: TF-IDF and Cosine Similarity - Match Q to Similar A\n",
    "\n",
    "### Overview\n",
    "\n",
    "__Part 2__ of the series shows the process of matching Questions to Answers based on the _Cosine Similarity_ of their _Term Frequency-Inverse Document Frequency (TF-IDF)_ matrix.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/experiment1_overview.PNG?token=APoO9gLiCcPVowvW0OsAuuXD9coI-HEsks5Ynhp0wA%3D%3D\">\n",
    "\n",
    "Note: This notebook series are built under Python 3.5 and NLTK 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from azure.storage import CloudStorageAccount\n",
    "from IPython.display import display\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read trainQ and testQ into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/trainQwithTokens.tsv'\n",
    "testQ_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/testQwithTokens.tsv'\n",
    "answersC_url = 'https://mezsa.blob.core.windows.net/stackoverflownew/answersCwithTokens.tsv'\n",
    "\n",
    "trainQ = pd.read_csv(trainQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_url, sep='\\t', index_col='Id', encoding='latin1')\n",
    "answersC = pd.read_csv(answersC_url, sep='\\t', index_col='Id', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokens to IDs Hash\n",
    "\n",
    "We assign an unique ID to each token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Token to ID mapping: {Token: tokenId}\n",
    "def tokens_to_ids(tokens, featureHash):\n",
    "    token2IdHash = {}\n",
    "    for i in range(len(tokens)):\n",
    "        tokenList = tokens.iloc[i].split(',')\n",
    "        if featureHash is None:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys():\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "        else:\n",
    "            for t in tokenList:\n",
    "                if t not in token2IdHash.keys() and t in list(featureHash.keys()):\n",
    "                    token2IdHash[t] = len(token2IdHash)\n",
    "            \n",
    "    return token2IdHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token2IdHashInit = tokens_to_ids(trainQ['Tokens'].append(answersC['Tokens']), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the TrainQ and Answers: 5133\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique tokens in the TrainQ and Answers: \" + str(len(token2IdHashInit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Count Matrix for Each Token in Each Question/Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_matrix(frame, token2IdHash, uniqueAnswerId):\n",
    "    # create am empty matrix with the shape of:\n",
    "    # num_row = num of unique tokens\n",
    "    # num_column = num of unique answerIds (N_wA) or num of questions\n",
    "    # rowIdx = token2IdHash.values()\n",
    "    # colIdx = index of uniqueAnswerId (N_wA) or index of questions\n",
    "    num_row = len(token2IdHash)\n",
    "    if uniqueAnswerId is not None:  # get N_wA\n",
    "        num_column = len(uniqueAnswerId)\n",
    "    else:\n",
    "        num_column = len(frame)\n",
    "    countMatrix = np.empty(shape=(num_row, num_column))\n",
    "\n",
    "    # loop through each question in the frame to fill in the countMatrix with corresponding counts\n",
    "    for i in range(len(frame)):\n",
    "        tokens = frame['Tokens'].iloc[i].split(',')\n",
    "        if uniqueAnswerId is not None:   # get N_wA\n",
    "            answerId = frame['AnswerId'].iloc[i]\n",
    "            colIdx = uniqueAnswerId.index(answerId)\n",
    "        else:  \n",
    "            colIdx = i\n",
    "            \n",
    "        for t in tokens:\n",
    "            if t in token2IdHash.keys():\n",
    "                rowIdx = token2IdHash[t]\n",
    "                countMatrix[rowIdx, colIdx] += 1\n",
    "\n",
    "    return countMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the count matrix of all training questions and answers.\n",
    "N_wQA = count_matrix(trainQ.append(answersC), token2IdHashInit, uniqueAnswerId=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute IDF Vector\n",
    "\n",
    "Considering all tokens observed in the training questions and answers, we compute their __Inverse Document Frequency__ based on the below formula.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/idf.PNG?token=APoO9qf3WptQgUPVRQJuOt4cobf56-Y3ks5YnhLSwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf(N_wQA):\n",
    "    # N is total number of documents in the corpus\n",
    "    # N_V is the number of tokens in the vocabulary\n",
    "    N_V, N = N_wQA.shape\n",
    "    # D is the number of documents where the token w appears\n",
    "    D = np.empty(shape=(0, N_V))\n",
    "    for i in range(N_V):\n",
    "        D = np.append(D, len(np.nonzero(N_wQA[i, ])[0]))\n",
    "    return np.log(N/D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = get_idf(N_wQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Normalized TF of Each Word w in Test and Answers Sets\n",
    "\n",
    "Each document d is typically represented by a feature vector x that represents the contents of d. Because different documents can have different lengths, it can be useful to apply L1 normalized feature vector x. Therefore, a normalized __Term Frequency__ matrix can be obtained based on the below formula.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/tf.PNG?token=APoO9tMyEVzqoUJYT9ALcdF3_BryHHEVks5YnIQywA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_tf(frame, token2IdHash):\n",
    "    N_w = count_matrix(frame, token2IdHash, uniqueAnswerId=None)\n",
    "    # get the column sum of the count matrix\n",
    "    N_W = np.sum(N_w, axis=0)\n",
    "    \n",
    "    # find the index where N_WQ is zero\n",
    "    zeroIdx = np.where(N_W == 0)[0]\n",
    "    \n",
    "    # if N_W is zero, then the x_w for that particular question/answer would be zero.\n",
    "    # for a simple calculation, we convert the N_WQ to 1 in those cases so the demoninator is not zero. \n",
    "    if len(zeroIdx) > 0:\n",
    "        N_W[zeroIdx] = 1\n",
    "    \n",
    "    # x_w = P_wd = count(w)/sum(count(i in V))\n",
    "    x_w = N_w / N_W\n",
    "    \n",
    "    return x_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate tf matrix of test question and answers independently.\n",
    "x_wTest = normalize_tf(testQ, token2IdHashInit)\n",
    "x_wAns = normalize_tf(answersC, token2IdHashInit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TF-IDF Matrix\n",
    "\n",
    "By knowing the __Term Frequency (TF)__ matrix and __Inverse Document Frequency (IDF)__ vector, we can simply compute __TF-IDF__ matrix by multiplying them together.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/tfidf.PNG?token=APoO9gw3rPhLusbG3if65TuVZNAnyqTCks5YnhWPwA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfTest = (x_wTest.T * idf).T\n",
    "tfidfAns = (x_wAns.T * idf).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cosine Similarity between Test Set and Answers\n",
    "\n",
    "For each question in the Test set, we compute its __Cosine Similarity__ against all answers in the Answers set. This similarity is a score that we use to consider how similar a question and an answer is. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/cosine.PNG?token=APoO9lOcKkP_7tFDh7p6KZXXVmokwLbGks5YnhY-wA%3D%3D\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def consine_similarity(tfidfL, tfidfR):\n",
    "    # calculate the dot product of two tfidf arrays\n",
    "    N = np.dot(tfidfL.T, tfidfR)\n",
    "    # calculate the norm of each tfidf array\n",
    "    normL = LA.norm(tfidfL, axis = 0)\n",
    "    normR = LA.norm(tfidfR, axis = 0)\n",
    "    similarity = (N.T/normL).T/normR\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate similarity scores of each question in Test set against all answers. \n",
    "simScores = consine_similarity(tfidfTest, tfidfAns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the Cosine Similarity and Calculate Average Rank \n",
    "\n",
    "We use two evaluation matrices to test our model performance. For each question in the test set, we calculate a __Cosine Similarity__ score against each answer. Then we rank the answers based on their __Cosine Similarity__ scores to calculate __Average Rank__ and __Top 10 Percentage__ in the Test set using the below formula:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Azure/Document_Matching/master/pic/evaluation.PNG?token=APoO9hyYDFxGc9FRbmIXU3VGv0wdeCaPks5YnIVtwA%3D%3D\">\n",
    "\n",
    "The __Average Rank__ can be interpreted as in average at which position we can find the correct answer among all available answers for a given question. \n",
    "\n",
    "The __Top 10 Percentage__ can be interpreted as how many percentage of the new questions that we can find their correct answers in the first 10 choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the similarity scores in descending order and map them to the corresponding AnswerId in Answer set\n",
    "def rank(frame, scores, uniqueAnswerId):\n",
    "    frame['SortedAnswers'] = list(np.array(uniqueAnswerId)[np.argsort(-scores, axis=1)])\n",
    "    \n",
    "    rankList = []\n",
    "    for i in range(len(frame)):\n",
    "        rankList.append(np.where(frame['SortedAnswers'].iloc[i] == frame['AnswerId'].iloc[i])[0][0] + 1)\n",
    "    frame['Rank'] = rankList\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get unique answerId in ascending order.\n",
    "uniqueAnswerId = answersC.index.values\n",
    "# calculate the rank of each question in Test set.\n",
    "testQ = rank(testQ, simScores, uniqueAnswerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of rank: 244.0\n",
      "Total number of questions in test set: 3468\n",
      "Total number of answers: 1201\n",
      "Total number of unique features: 5133\n",
      "Percentage of questions find answers in top 10: 0.202\n"
     ]
    }
   ],
   "source": [
    "# average of rank\n",
    "print('Average of rank: ' + str(np.floor(testQ['Rank'].mean())))\n",
    "print('Total number of questions in test set: ' + str(len(testQ)))\n",
    "print('Total number of answers: ' + str(len(uniqueAnswerId)))\n",
    "print('Total number of unique features: ' + str(len(token2IdHashInit)))\n",
    "print('Percentage of questions find answers in top 10: ' + str(round(len(testQ.query('Rank <= 10'))/len(testQ), 3)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
